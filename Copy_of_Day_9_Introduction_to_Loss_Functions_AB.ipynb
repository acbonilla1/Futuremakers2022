{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Day_9_Introduction_to_Loss_Functions_AB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acbonilla1/Futuremakers2022/blob/main/Copy_of_Day_9_Introduction_to_Loss_Functions_AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100a75cb-d35b-4224-8eab-6a423b264d7d"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f6e9be-a848-47e1-b760-ff359d6962b9"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 597.9567 - mse: 597.9567 - val_loss: 498.7641 - val_mse: 498.7641\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 579.6784 - mse: 579.6784 - val_loss: 481.5521 - val_mse: 481.5521\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 559.2590 - mse: 559.2590 - val_loss: 459.1263 - val_mse: 459.1263\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 531.1355 - mse: 531.1355 - val_loss: 425.8991 - val_mse: 425.8991\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 488.5981 - mse: 488.5981 - val_loss: 378.2169 - val_mse: 378.2169\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 428.8337 - mse: 428.8337 - val_loss: 311.7227 - val_mse: 311.7227\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 346.4290 - mse: 346.4290 - val_loss: 226.6918 - val_mse: 226.6918\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 245.2012 - mse: 245.2012 - val_loss: 134.9840 - val_mse: 134.9840\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 147.1536 - mse: 147.1536 - val_loss: 65.7285 - val_mse: 65.7285\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 82.2100 - mse: 82.2100 - val_loss: 49.0892 - val_mse: 49.0892\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 66.4376 - mse: 66.4376 - val_loss: 50.5402 - val_mse: 50.5402\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 56.8789 - mse: 56.8789 - val_loss: 41.7803 - val_mse: 41.7803\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 48.5801 - mse: 48.5801 - val_loss: 36.6041 - val_mse: 36.6041\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 43.3726 - mse: 43.3726 - val_loss: 32.7135 - val_mse: 32.7135\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 39.0695 - mse: 39.0695 - val_loss: 31.3319 - val_mse: 31.3319\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 36.0334 - mse: 36.0334 - val_loss: 29.8802 - val_mse: 29.8802\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 33.7640 - mse: 33.7640 - val_loss: 27.9723 - val_mse: 27.9723\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 31.5782 - mse: 31.5782 - val_loss: 27.1052 - val_mse: 27.1052\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 30.1571 - mse: 30.1571 - val_loss: 24.7861 - val_mse: 24.7861\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28.6812 - mse: 28.6812 - val_loss: 24.6275 - val_mse: 24.6275\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27.4650 - mse: 27.4650 - val_loss: 24.4001 - val_mse: 24.4001\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26.4365 - mse: 26.4365 - val_loss: 23.9017 - val_mse: 23.9017\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 25.6462 - mse: 25.6462 - val_loss: 23.0466 - val_mse: 23.0466\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 24.6743 - mse: 24.6743 - val_loss: 22.9770 - val_mse: 22.9770\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 24.0708 - mse: 24.0708 - val_loss: 22.7303 - val_mse: 22.7303\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 23.5908 - mse: 23.5908 - val_loss: 22.6583 - val_mse: 22.6583\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.9714 - mse: 22.9714 - val_loss: 21.7230 - val_mse: 21.7230\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 22.4781 - mse: 22.4781 - val_loss: 22.1787 - val_mse: 22.1787\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.9340 - mse: 21.9340 - val_loss: 21.5237 - val_mse: 21.5237\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.5196 - mse: 21.5196 - val_loss: 21.6998 - val_mse: 21.6998\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.1242 - mse: 21.1242 - val_loss: 21.4060 - val_mse: 21.4060\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.7721 - mse: 20.7721 - val_loss: 20.9934 - val_mse: 20.9934\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.4049 - mse: 20.4049 - val_loss: 21.0818 - val_mse: 21.0818\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.1476 - mse: 20.1476 - val_loss: 21.0576 - val_mse: 21.0576\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.8227 - mse: 19.8227 - val_loss: 20.5569 - val_mse: 20.5569\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.5466 - mse: 19.5466 - val_loss: 20.0513 - val_mse: 20.0513\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.2687 - mse: 19.2687 - val_loss: 20.1659 - val_mse: 20.1659\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.9857 - mse: 18.9857 - val_loss: 20.4145 - val_mse: 20.4145\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.7313 - mse: 18.7313 - val_loss: 19.8946 - val_mse: 19.8946\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.4425 - mse: 18.4425 - val_loss: 19.3015 - val_mse: 19.3015\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.2156 - mse: 18.2156 - val_loss: 19.4822 - val_mse: 19.4822\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.9840 - mse: 17.9840 - val_loss: 19.2921 - val_mse: 19.2921\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.7724 - mse: 17.7724 - val_loss: 19.0118 - val_mse: 19.0118\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.5707 - mse: 17.5707 - val_loss: 19.0588 - val_mse: 19.0588\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.2939 - mse: 17.2939 - val_loss: 18.6026 - val_mse: 18.6026\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.1721 - mse: 17.1721 - val_loss: 18.5127 - val_mse: 18.5127\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.9354 - mse: 16.9354 - val_loss: 18.0089 - val_mse: 18.0089\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.8165 - mse: 16.8165 - val_loss: 18.1974 - val_mse: 18.1974\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.4552 - mse: 16.4552 - val_loss: 17.4897 - val_mse: 17.4897\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.2978 - mse: 16.2978 - val_loss: 17.6963 - val_mse: 17.6963\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.1381 - mse: 16.1381 - val_loss: 17.4995 - val_mse: 17.4995\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.9018 - mse: 15.9018 - val_loss: 17.1718 - val_mse: 17.1718\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.7515 - mse: 15.7515 - val_loss: 17.1889 - val_mse: 17.1889\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.7246 - mse: 15.7246 - val_loss: 17.2999 - val_mse: 17.2999\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.5272 - mse: 15.5272 - val_loss: 16.5590 - val_mse: 16.5590\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.2288 - mse: 15.2288 - val_loss: 16.8918 - val_mse: 16.8918\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.2091 - mse: 15.2091 - val_loss: 16.9630 - val_mse: 16.9630\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.9578 - mse: 14.9578 - val_loss: 16.4484 - val_mse: 16.4484\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.8191 - mse: 14.8191 - val_loss: 16.3482 - val_mse: 16.3482\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.6975 - mse: 14.6975 - val_loss: 16.2227 - val_mse: 16.2227\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.6350 - mse: 14.6350 - val_loss: 15.8225 - val_mse: 15.8225\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.4259 - mse: 14.4259 - val_loss: 15.9758 - val_mse: 15.9758\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.3205 - mse: 14.3205 - val_loss: 15.6140 - val_mse: 15.6140\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.2314 - mse: 14.2314 - val_loss: 15.7810 - val_mse: 15.7810\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.1366 - mse: 14.1366 - val_loss: 15.5733 - val_mse: 15.5733\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.9681 - mse: 13.9681 - val_loss: 15.5847 - val_mse: 15.5847\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.9221 - mse: 13.9221 - val_loss: 15.0947 - val_mse: 15.0947\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.6951 - mse: 13.6951 - val_loss: 15.3709 - val_mse: 15.3709\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5953 - mse: 13.5953 - val_loss: 14.8408 - val_mse: 14.8408\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5044 - mse: 13.5044 - val_loss: 14.7185 - val_mse: 14.7185\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.4417 - mse: 13.4417 - val_loss: 15.0546 - val_mse: 15.0546\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.2809 - mse: 13.2809 - val_loss: 14.5059 - val_mse: 14.5059\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.2822 - mse: 13.2822 - val_loss: 14.3391 - val_mse: 14.3391\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.1141 - mse: 13.1141 - val_loss: 14.3512 - val_mse: 14.3512\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.0507 - mse: 13.0507 - val_loss: 13.9624 - val_mse: 13.9624\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.0725 - mse: 13.0725 - val_loss: 13.8259 - val_mse: 13.8259\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.0088 - mse: 13.0088 - val_loss: 14.2634 - val_mse: 14.2634\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.8659 - mse: 12.8659 - val_loss: 13.4056 - val_mse: 13.4056\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.8580 - mse: 12.8580 - val_loss: 13.7050 - val_mse: 13.7050\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.6362 - mse: 12.6362 - val_loss: 13.2805 - val_mse: 13.2805\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.3844 - mse: 12.3844 - val_loss: 13.4112 - val_mse: 13.4112\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.3711 - mse: 12.3711 - val_loss: 13.1230 - val_mse: 13.1230\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.3967 - mse: 12.3967 - val_loss: 13.2164 - val_mse: 13.2164\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.3266 - mse: 12.3266 - val_loss: 12.7696 - val_mse: 12.7696\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.0958 - mse: 12.0958 - val_loss: 13.0603 - val_mse: 13.0603\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.0671 - mse: 12.0671 - val_loss: 12.6331 - val_mse: 12.6331\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.9894 - mse: 11.9894 - val_loss: 12.5143 - val_mse: 12.5143\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.8892 - mse: 11.8892 - val_loss: 12.5794 - val_mse: 12.5794\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.9567 - mse: 11.9567 - val_loss: 12.4281 - val_mse: 12.4281\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7930 - mse: 11.7930 - val_loss: 12.2041 - val_mse: 12.2041\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.8828 - mse: 11.8828 - val_loss: 12.0022 - val_mse: 12.0022\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.6492 - mse: 11.6492 - val_loss: 11.8997 - val_mse: 11.8997\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5785 - mse: 11.5785 - val_loss: 11.9058 - val_mse: 11.9058\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.5084 - mse: 11.5084 - val_loss: 11.8428 - val_mse: 11.8428\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.5099 - mse: 11.5099 - val_loss: 11.7480 - val_mse: 11.7480\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.4462 - mse: 11.4462 - val_loss: 11.7212 - val_mse: 11.7212\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5326 - mse: 11.5326 - val_loss: 11.5719 - val_mse: 11.5719\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.3069 - mse: 11.3069 - val_loss: 11.7520 - val_mse: 11.7520\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.3114 - mse: 11.3114 - val_loss: 11.2952 - val_mse: 11.2952\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.2964 - mse: 11.2964 - val_loss: 11.3769 - val_mse: 11.3769\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.1269 - mse: 11.1269 - val_loss: 11.1206 - val_mse: 11.1206\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.4633 - mse: 11.4633 - val_loss: 11.1978 - val_mse: 11.1978\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.5395 - mse: 11.5395 - val_loss: 10.7740 - val_mse: 10.7740\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.0967 - mse: 11.0967 - val_loss: 11.2835 - val_mse: 11.2835\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.1738 - mse: 11.1738 - val_loss: 10.6959 - val_mse: 10.6959\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.0700 - mse: 11.0700 - val_loss: 11.0522 - val_mse: 11.0522\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.8236 - mse: 10.8236 - val_loss: 10.7092 - val_mse: 10.7092\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7240 - mse: 10.7240 - val_loss: 10.6952 - val_mse: 10.6952\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7427 - mse: 10.7427 - val_loss: 10.5908 - val_mse: 10.5908\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.6184 - mse: 10.6184 - val_loss: 10.6702 - val_mse: 10.6702\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6968 - mse: 10.6968 - val_loss: 10.4557 - val_mse: 10.4557\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5533 - mse: 10.5533 - val_loss: 10.4241 - val_mse: 10.4241\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.4831 - mse: 10.4831 - val_loss: 10.3975 - val_mse: 10.3975\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.4861 - mse: 10.4861 - val_loss: 10.3423 - val_mse: 10.3423\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3902 - mse: 10.3902 - val_loss: 10.2966 - val_mse: 10.2966\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3776 - mse: 10.3776 - val_loss: 10.1233 - val_mse: 10.1233\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.3612 - mse: 10.3612 - val_loss: 9.9904 - val_mse: 9.9904\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3968 - mse: 10.3968 - val_loss: 10.0234 - val_mse: 10.0234\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5380 - mse: 10.5380 - val_loss: 10.0504 - val_mse: 10.0504\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.3157 - mse: 10.3157 - val_loss: 9.8444 - val_mse: 9.8444\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1806 - mse: 10.1806 - val_loss: 9.9080 - val_mse: 9.9080\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2695 - mse: 10.2695 - val_loss: 9.6136 - val_mse: 9.6136\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.1550 - mse: 10.1550 - val_loss: 9.8785 - val_mse: 9.8785\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2398 - mse: 10.2398 - val_loss: 9.6432 - val_mse: 9.6432\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.0047 - mse: 10.0047 - val_loss: 9.6680 - val_mse: 9.6680\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9222 - mse: 9.9222 - val_loss: 9.5815 - val_mse: 9.5815\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.0197 - mse: 10.0197 - val_loss: 9.4432 - val_mse: 9.4432\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8077 - mse: 9.8077 - val_loss: 9.6194 - val_mse: 9.6194\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8091 - mse: 9.8091 - val_loss: 9.3144 - val_mse: 9.3144\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8129 - mse: 9.8129 - val_loss: 9.3558 - val_mse: 9.3558\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7296 - mse: 9.7296 - val_loss: 9.3858 - val_mse: 9.3858\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6731 - mse: 9.6731 - val_loss: 9.2716 - val_mse: 9.2716\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7123 - mse: 9.7123 - val_loss: 9.3141 - val_mse: 9.3141\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.5483 - mse: 9.5483 - val_loss: 9.3408 - val_mse: 9.3408\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6470 - mse: 9.6470 - val_loss: 9.2016 - val_mse: 9.2016\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8164 - mse: 9.8164 - val_loss: 9.1100 - val_mse: 9.1100\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6214 - mse: 9.6214 - val_loss: 9.0958 - val_mse: 9.0958\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4806 - mse: 9.4806 - val_loss: 9.0743 - val_mse: 9.0743\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4536 - mse: 9.4536 - val_loss: 9.1510 - val_mse: 9.1510\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3802 - mse: 9.3802 - val_loss: 9.0148 - val_mse: 9.0148\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2791 - mse: 9.2791 - val_loss: 9.0246 - val_mse: 9.0246\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.2520 - mse: 9.2520 - val_loss: 9.1310 - val_mse: 9.1310\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.2422 - mse: 9.2422 - val_loss: 9.0356 - val_mse: 9.0356\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1512 - mse: 9.1512 - val_loss: 9.0006 - val_mse: 9.0006\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1446 - mse: 9.1446 - val_loss: 8.8841 - val_mse: 8.8841\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1234 - mse: 9.1234 - val_loss: 8.9351 - val_mse: 8.9351\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0112 - mse: 9.0112 - val_loss: 8.8368 - val_mse: 8.8368\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9855 - mse: 8.9855 - val_loss: 8.7262 - val_mse: 8.7262\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9388 - mse: 8.9388 - val_loss: 8.6909 - val_mse: 8.6909\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8986 - mse: 8.8986 - val_loss: 8.6381 - val_mse: 8.6381\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8698 - mse: 8.8698 - val_loss: 8.5710 - val_mse: 8.5710\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8002 - mse: 8.8002 - val_loss: 8.6166 - val_mse: 8.6166\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7455 - mse: 8.7455 - val_loss: 8.5360 - val_mse: 8.5360\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7187 - mse: 8.7187 - val_loss: 8.5118 - val_mse: 8.5118\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7303 - mse: 8.7303 - val_loss: 8.4855 - val_mse: 8.4855\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6155 - mse: 8.6155 - val_loss: 8.4349 - val_mse: 8.4349\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8287 - mse: 8.8287 - val_loss: 8.4308 - val_mse: 8.4308\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6525 - mse: 8.6525 - val_loss: 8.4568 - val_mse: 8.4568\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5741 - mse: 8.5741 - val_loss: 8.3402 - val_mse: 8.3402\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4282 - mse: 8.4282 - val_loss: 8.4642 - val_mse: 8.4642\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4127 - mse: 8.4127 - val_loss: 8.2408 - val_mse: 8.2408\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3044 - mse: 8.3044 - val_loss: 8.2584 - val_mse: 8.2584\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3012 - mse: 8.3012 - val_loss: 8.2270 - val_mse: 8.2270\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2983 - mse: 8.2983 - val_loss: 8.1670 - val_mse: 8.1670\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2563 - mse: 8.2563 - val_loss: 8.0534 - val_mse: 8.0534\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4181 - mse: 8.4181 - val_loss: 8.2305 - val_mse: 8.2305\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1954 - mse: 8.1954 - val_loss: 8.1647 - val_mse: 8.1647\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0671 - mse: 8.0671 - val_loss: 8.0592 - val_mse: 8.0592\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0344 - mse: 8.0344 - val_loss: 8.0741 - val_mse: 8.0741\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0141 - mse: 8.0141 - val_loss: 8.0089 - val_mse: 8.0089\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9590 - mse: 7.9590 - val_loss: 7.9635 - val_mse: 7.9635\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0622 - mse: 8.0622 - val_loss: 8.1045 - val_mse: 8.1045\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8346 - mse: 7.8346 - val_loss: 7.8262 - val_mse: 7.8262\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8454 - mse: 7.8454 - val_loss: 7.8430 - val_mse: 7.8430\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8169 - mse: 7.8169 - val_loss: 7.8663 - val_mse: 7.8663\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8001 - mse: 7.8001 - val_loss: 7.8219 - val_mse: 7.8219\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7424 - mse: 7.7424 - val_loss: 7.7836 - val_mse: 7.7836\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7551 - mse: 7.7551 - val_loss: 7.7137 - val_mse: 7.7137\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6314 - mse: 7.6314 - val_loss: 7.7367 - val_mse: 7.7367\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6379 - mse: 7.6379 - val_loss: 7.6489 - val_mse: 7.6489\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6454 - mse: 7.6454 - val_loss: 7.6201 - val_mse: 7.6201\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4804 - mse: 7.4804 - val_loss: 7.7794 - val_mse: 7.7794\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4737 - mse: 7.4737 - val_loss: 7.6647 - val_mse: 7.6647\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5044 - mse: 7.5044 - val_loss: 7.5873 - val_mse: 7.5873\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4251 - mse: 7.4251 - val_loss: 7.5272 - val_mse: 7.5272\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4009 - mse: 7.4009 - val_loss: 7.7599 - val_mse: 7.7599\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4079 - mse: 7.4079 - val_loss: 7.6247 - val_mse: 7.6247\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2977 - mse: 7.2977 - val_loss: 7.4270 - val_mse: 7.4270\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3136 - mse: 7.3136 - val_loss: 7.5069 - val_mse: 7.5069\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2865 - mse: 7.2865 - val_loss: 7.4453 - val_mse: 7.4453\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1462 - mse: 7.1462 - val_loss: 7.4008 - val_mse: 7.4008\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1609 - mse: 7.1609 - val_loss: 7.3851 - val_mse: 7.3851\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1173 - mse: 7.1173 - val_loss: 7.4376 - val_mse: 7.4376\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2383 - mse: 7.2383 - val_loss: 7.6712 - val_mse: 7.6712\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0776 - mse: 7.0776 - val_loss: 7.4995 - val_mse: 7.4995\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0595 - mse: 7.0595 - val_loss: 7.5069 - val_mse: 7.5069\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0747 - mse: 7.0747 - val_loss: 7.5070 - val_mse: 7.5070\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0324 - mse: 7.0324 - val_loss: 7.5559 - val_mse: 7.5559\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0525 - mse: 7.0525 - val_loss: 7.5972 - val_mse: 7.5972\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0801 - mse: 7.0801 - val_loss: 7.4790 - val_mse: 7.4790\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0308 - mse: 7.0308 - val_loss: 7.4541 - val_mse: 7.4541\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9184 - mse: 6.9184 - val_loss: 7.6043 - val_mse: 7.6043\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8761 - mse: 6.8761 - val_loss: 7.3593 - val_mse: 7.3593\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8900 - mse: 6.8900 - val_loss: 7.4278 - val_mse: 7.4278\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7516 - mse: 6.7516 - val_loss: 7.3469 - val_mse: 7.3469\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6860 - mse: 6.6860 - val_loss: 7.4259 - val_mse: 7.4259\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7194 - mse: 6.7194 - val_loss: 7.4263 - val_mse: 7.4263\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7121 - mse: 6.7121 - val_loss: 7.2858 - val_mse: 7.2858\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6862 - mse: 6.6862 - val_loss: 7.2884 - val_mse: 7.2884\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5923 - mse: 6.5923 - val_loss: 7.4096 - val_mse: 7.4096\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6006 - mse: 6.6006 - val_loss: 7.2401 - val_mse: 7.2401\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5304 - mse: 6.5304 - val_loss: 7.2890 - val_mse: 7.2890\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5213 - mse: 6.5213 - val_loss: 7.3500 - val_mse: 7.3500\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4743 - mse: 6.4743 - val_loss: 7.3361 - val_mse: 7.3361\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7518 - mse: 6.7518 - val_loss: 7.3120 - val_mse: 7.3120\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4361 - mse: 6.4361 - val_loss: 7.4188 - val_mse: 7.4188\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4939 - mse: 6.4939 - val_loss: 7.3355 - val_mse: 7.3355\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4431 - mse: 6.4431 - val_loss: 7.3388 - val_mse: 7.3388\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3540 - mse: 6.3540 - val_loss: 7.3230 - val_mse: 7.3230\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3371 - mse: 6.3371 - val_loss: 7.5963 - val_mse: 7.5963\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3331 - mse: 6.3331 - val_loss: 7.4599 - val_mse: 7.4599\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2832 - mse: 6.2832 - val_loss: 7.3847 - val_mse: 7.3847\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3137 - mse: 6.3137 - val_loss: 7.4792 - val_mse: 7.4792\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2613 - mse: 6.2613 - val_loss: 7.4663 - val_mse: 7.4663\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2808 - mse: 6.2808 - val_loss: 7.4796 - val_mse: 7.4796\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2195 - mse: 6.2195 - val_loss: 7.5830 - val_mse: 7.5830\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3458 - mse: 6.3458 - val_loss: 7.4317 - val_mse: 7.4317\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2354 - mse: 6.2354 - val_loss: 7.6334 - val_mse: 7.6334\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3148 - mse: 6.3148 - val_loss: 7.4797 - val_mse: 7.4797\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2207 - mse: 6.2207 - val_loss: 7.4071 - val_mse: 7.4071\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2319 - mse: 6.2319 - val_loss: 7.5404 - val_mse: 7.5404\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0848 - mse: 6.0848 - val_loss: 7.5230 - val_mse: 7.5230\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1404 - mse: 6.1404 - val_loss: 7.4098 - val_mse: 7.4098\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0258 - mse: 6.0258 - val_loss: 7.6345 - val_mse: 7.6345\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1038 - mse: 6.1038 - val_loss: 7.3781 - val_mse: 7.3781\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0436 - mse: 6.0436 - val_loss: 7.4602 - val_mse: 7.4602\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9436 - mse: 5.9436 - val_loss: 7.4329 - val_mse: 7.4329\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9863 - mse: 5.9863 - val_loss: 7.4648 - val_mse: 7.4648\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0058 - mse: 6.0058 - val_loss: 7.3774 - val_mse: 7.3774\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9705 - mse: 5.9705 - val_loss: 7.4652 - val_mse: 7.4652\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8892 - mse: 5.8892 - val_loss: 7.5655 - val_mse: 7.5655\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9064 - mse: 5.9064 - val_loss: 7.6843 - val_mse: 7.6843\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8621 - mse: 5.8621 - val_loss: 7.6197 - val_mse: 7.6197\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9246 - mse: 5.9246 - val_loss: 7.5207 - val_mse: 7.5207\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8356 - mse: 5.8356 - val_loss: 7.3739 - val_mse: 7.3739\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8159 - mse: 5.8159 - val_loss: 7.5592 - val_mse: 7.5592\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7817 - mse: 5.7817 - val_loss: 7.7473 - val_mse: 7.7473\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8286 - mse: 5.8286 - val_loss: 7.5310 - val_mse: 7.5310\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8147 - mse: 5.8147 - val_loss: 7.5539 - val_mse: 7.5539\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7722 - mse: 5.7722 - val_loss: 7.5449 - val_mse: 7.5449\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e665a4110>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "d392419b-6144-4da6-85cd-219b78bb28ee"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "errors = predictions - test_labels\n",
        "\n",
        "mse = sum((predictions - test_labels) ** 2) / len(predictions) # Enter your code here\n",
        "\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd847a7-00be-4f4f-c2f7-ba5f0af1497c"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 14ms/step - loss: 22.2020 - mae: 22.2020 - val_loss: 20.6135 - val_mae: 20.6135\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.9852 - mae: 20.9852 - val_loss: 18.9554 - val_mae: 18.9554\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.8682 - mae: 18.8682 - val_loss: 16.2788 - val_mae: 16.2788\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.5207 - mae: 15.5207 - val_loss: 12.3852 - val_mae: 12.3852\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.1429 - mae: 11.1429 - val_loss: 7.3806 - val_mae: 7.3806\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2075 - mae: 7.2075 - val_loss: 5.9103 - val_mae: 5.9103\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3757 - mae: 6.3757 - val_loss: 4.8023 - val_mae: 4.8023\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8799 - mae: 4.8799 - val_loss: 3.5743 - val_mae: 3.5743\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.9371 - mae: 3.9371 - val_loss: 3.4908 - val_mae: 3.4908\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.4214 - mae: 3.4214 - val_loss: 3.2650 - val_mae: 3.2650\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.1437 - mae: 3.1437 - val_loss: 3.0322 - val_mae: 3.0322\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.9806 - mae: 2.9806 - val_loss: 2.8273 - val_mae: 2.8273\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.8271 - mae: 2.8271 - val_loss: 2.7605 - val_mae: 2.7605\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.7031 - mae: 2.7031 - val_loss: 2.7364 - val_mae: 2.7364\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6340 - mae: 2.6340 - val_loss: 2.6274 - val_mae: 2.6274\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.5625 - mae: 2.5625 - val_loss: 2.5717 - val_mae: 2.5717\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4847 - mae: 2.4847 - val_loss: 2.5670 - val_mae: 2.5670\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4878 - mae: 2.4878 - val_loss: 2.4361 - val_mae: 2.4361\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4490 - mae: 2.4490 - val_loss: 2.6469 - val_mae: 2.6469\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3891 - mae: 2.3891 - val_loss: 2.3751 - val_mae: 2.3751\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3660 - mae: 2.3660 - val_loss: 2.5472 - val_mae: 2.5472\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3350 - mae: 2.3350 - val_loss: 2.4264 - val_mae: 2.4264\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3078 - mae: 2.3078 - val_loss: 2.4274 - val_mae: 2.4274\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2816 - mae: 2.2816 - val_loss: 2.4601 - val_mae: 2.4601\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.2674 - mae: 2.2674 - val_loss: 2.4276 - val_mae: 2.4276\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2431 - mae: 2.2431 - val_loss: 2.3864 - val_mae: 2.3864\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1991 - mae: 2.1991 - val_loss: 2.4442 - val_mae: 2.4442\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.2068 - mae: 2.2068 - val_loss: 2.3693 - val_mae: 2.3693\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1838 - mae: 2.1838 - val_loss: 2.3393 - val_mae: 2.3393\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1762 - mae: 2.1762 - val_loss: 2.5268 - val_mae: 2.5268\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1713 - mae: 2.1713 - val_loss: 2.3330 - val_mae: 2.3330\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1897 - mae: 2.1897 - val_loss: 2.4895 - val_mae: 2.4895\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1343 - mae: 2.1343 - val_loss: 2.2466 - val_mae: 2.2466\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1117 - mae: 2.1117 - val_loss: 2.3304 - val_mae: 2.3304\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0590 - mae: 2.0590 - val_loss: 2.3861 - val_mae: 2.3861\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0480 - mae: 2.0480 - val_loss: 2.2997 - val_mae: 2.2997\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0560 - mae: 2.0560 - val_loss: 2.3099 - val_mae: 2.3099\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0123 - mae: 2.0123 - val_loss: 2.2541 - val_mae: 2.2541\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0019 - mae: 2.0019 - val_loss: 2.2452 - val_mae: 2.2452\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0289 - mae: 2.0289 - val_loss: 2.4327 - val_mae: 2.4327\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0132 - mae: 2.0132 - val_loss: 2.2600 - val_mae: 2.2600\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9617 - mae: 1.9617 - val_loss: 2.2406 - val_mae: 2.2406\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9558 - mae: 1.9558 - val_loss: 2.2000 - val_mae: 2.2000\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9477 - mae: 1.9477 - val_loss: 2.2124 - val_mae: 2.2124\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9214 - mae: 1.9214 - val_loss: 2.2033 - val_mae: 2.2033\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9131 - mae: 1.9131 - val_loss: 2.2454 - val_mae: 2.2454\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9228 - mae: 1.9228 - val_loss: 2.2682 - val_mae: 2.2682\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9348 - mae: 1.9348 - val_loss: 2.2526 - val_mae: 2.2526\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9413 - mae: 1.9413 - val_loss: 2.1848 - val_mae: 2.1848\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8791 - mae: 1.8791 - val_loss: 2.1913 - val_mae: 2.1913\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8770 - mae: 1.8770 - val_loss: 2.1420 - val_mae: 2.1420\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8433 - mae: 1.8433 - val_loss: 2.2018 - val_mae: 2.2018\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8671 - mae: 1.8671 - val_loss: 2.2572 - val_mae: 2.2572\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8609 - mae: 1.8609 - val_loss: 2.1482 - val_mae: 2.1482\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8419 - mae: 1.8419 - val_loss: 2.2190 - val_mae: 2.2190\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8112 - mae: 1.8112 - val_loss: 2.1582 - val_mae: 2.1582\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8124 - mae: 1.8124 - val_loss: 2.2163 - val_mae: 2.2163\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8025 - mae: 1.8025 - val_loss: 2.2407 - val_mae: 2.2407\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7703 - mae: 1.7703 - val_loss: 2.2734 - val_mae: 2.2734\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7981 - mae: 1.7981 - val_loss: 2.2224 - val_mae: 2.2224\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7651 - mae: 1.7651 - val_loss: 2.1913 - val_mae: 2.1913\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7639 - mae: 1.7639 - val_loss: 2.3307 - val_mae: 2.3307\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7634 - mae: 1.7634 - val_loss: 2.1958 - val_mae: 2.1958\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7746 - mae: 1.7746 - val_loss: 2.2907 - val_mae: 2.2907\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7909 - mae: 1.7909 - val_loss: 2.1702 - val_mae: 2.1702\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7343 - mae: 1.7343 - val_loss: 2.1652 - val_mae: 2.1652\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7299 - mae: 1.7299 - val_loss: 2.3117 - val_mae: 2.3117\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7350 - mae: 1.7350 - val_loss: 2.1280 - val_mae: 2.1280\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7024 - mae: 1.7024 - val_loss: 2.2075 - val_mae: 2.2075\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7129 - mae: 1.7129 - val_loss: 2.2290 - val_mae: 2.2290\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7774 - mae: 1.7774 - val_loss: 2.1616 - val_mae: 2.1616\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7126 - mae: 1.7126 - val_loss: 2.2676 - val_mae: 2.2676\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7381 - mae: 1.7381 - val_loss: 2.1555 - val_mae: 2.1555\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6948 - mae: 1.6948 - val_loss: 2.1095 - val_mae: 2.1095\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7333 - mae: 1.7333 - val_loss: 2.2414 - val_mae: 2.2414\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6888 - mae: 1.6888 - val_loss: 2.1684 - val_mae: 2.1684\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7104 - mae: 1.7104 - val_loss: 2.3155 - val_mae: 2.3155\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7444 - mae: 1.7444 - val_loss: 2.1255 - val_mae: 2.1255\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6293 - mae: 1.6293 - val_loss: 2.2107 - val_mae: 2.2107\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6348 - mae: 1.6348 - val_loss: 2.1023 - val_mae: 2.1023\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6436 - mae: 1.6436 - val_loss: 2.0890 - val_mae: 2.0890\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5988 - mae: 1.5988 - val_loss: 2.1433 - val_mae: 2.1433\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6082 - mae: 1.6082 - val_loss: 2.1365 - val_mae: 2.1365\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5846 - mae: 1.5846 - val_loss: 2.0884 - val_mae: 2.0884\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5956 - mae: 1.5956 - val_loss: 2.1187 - val_mae: 2.1187\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6031 - mae: 1.6031 - val_loss: 2.0686 - val_mae: 2.0686\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6353 - mae: 1.6353 - val_loss: 2.0845 - val_mae: 2.0845\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6262 - mae: 1.6262 - val_loss: 2.1574 - val_mae: 2.1574\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5865 - mae: 1.5865 - val_loss: 2.0982 - val_mae: 2.0982\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5660 - mae: 1.5660 - val_loss: 2.1404 - val_mae: 2.1404\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5427 - mae: 1.5427 - val_loss: 2.1010 - val_mae: 2.1010\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5179 - mae: 1.5179 - val_loss: 2.0640 - val_mae: 2.0640\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5255 - mae: 1.5255 - val_loss: 2.0588 - val_mae: 2.0588\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5620 - mae: 1.5620 - val_loss: 2.0924 - val_mae: 2.0924\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5325 - mae: 1.5325 - val_loss: 2.0908 - val_mae: 2.0908\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5477 - mae: 1.5477 - val_loss: 2.0088 - val_mae: 2.0088\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5395 - mae: 1.5395 - val_loss: 2.0719 - val_mae: 2.0719\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5511 - mae: 1.5511 - val_loss: 2.0900 - val_mae: 2.0900\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5120 - mae: 1.5120 - val_loss: 2.0523 - val_mae: 2.0523\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5104 - mae: 1.5104 - val_loss: 2.0847 - val_mae: 2.0847\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5262 - mae: 1.5262 - val_loss: 2.0571 - val_mae: 2.0571\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5098 - mae: 1.5098 - val_loss: 2.1071 - val_mae: 2.1071\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4876 - mae: 1.4876 - val_loss: 2.0661 - val_mae: 2.0661\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5112 - mae: 1.5112 - val_loss: 2.0408 - val_mae: 2.0408\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4428 - mae: 1.4428 - val_loss: 2.1089 - val_mae: 2.1089\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5519 - mae: 1.5519 - val_loss: 2.0438 - val_mae: 2.0438\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5608 - mae: 1.5608 - val_loss: 2.1204 - val_mae: 2.1204\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5009 - mae: 1.5009 - val_loss: 1.9995 - val_mae: 1.9995\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5028 - mae: 1.5028 - val_loss: 2.0934 - val_mae: 2.0934\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4319 - mae: 1.4319 - val_loss: 2.0399 - val_mae: 2.0399\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4531 - mae: 1.4531 - val_loss: 2.0279 - val_mae: 2.0279\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4597 - mae: 1.4597 - val_loss: 2.0492 - val_mae: 2.0492\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4372 - mae: 1.4372 - val_loss: 2.0843 - val_mae: 2.0843\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4779 - mae: 1.4779 - val_loss: 2.0593 - val_mae: 2.0593\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4135 - mae: 1.4135 - val_loss: 2.0580 - val_mae: 2.0580\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4183 - mae: 1.4183 - val_loss: 2.0689 - val_mae: 2.0689\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3811 - mae: 1.3811 - val_loss: 2.0384 - val_mae: 2.0384\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3818 - mae: 1.3818 - val_loss: 2.0139 - val_mae: 2.0139\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3909 - mae: 1.3909 - val_loss: 2.0643 - val_mae: 2.0643\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3661 - mae: 1.3661 - val_loss: 2.0313 - val_mae: 2.0313\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3628 - mae: 1.3628 - val_loss: 2.0410 - val_mae: 2.0410\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3819 - mae: 1.3819 - val_loss: 2.0737 - val_mae: 2.0737\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4651 - mae: 1.4651 - val_loss: 2.0562 - val_mae: 2.0562\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3848 - mae: 1.3848 - val_loss: 2.0637 - val_mae: 2.0637\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3413 - mae: 1.3413 - val_loss: 2.0635 - val_mae: 2.0635\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3326 - mae: 1.3326 - val_loss: 2.0245 - val_mae: 2.0245\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3364 - mae: 1.3364 - val_loss: 2.0428 - val_mae: 2.0428\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3577 - mae: 1.3577 - val_loss: 2.1191 - val_mae: 2.1191\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3782 - mae: 1.3782 - val_loss: 2.0415 - val_mae: 2.0415\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3813 - mae: 1.3813 - val_loss: 2.0889 - val_mae: 2.0889\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4062 - mae: 1.4062 - val_loss: 2.1138 - val_mae: 2.1138\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3580 - mae: 1.3580 - val_loss: 2.0644 - val_mae: 2.0644\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3319 - mae: 1.3319 - val_loss: 2.0650 - val_mae: 2.0650\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3060 - mae: 1.3060 - val_loss: 2.0933 - val_mae: 2.0933\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3978 - mae: 1.3978 - val_loss: 2.0531 - val_mae: 2.0531\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3435 - mae: 1.3435 - val_loss: 2.1469 - val_mae: 2.1469\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3301 - mae: 1.3301 - val_loss: 2.0375 - val_mae: 2.0375\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3535 - mae: 1.3535 - val_loss: 2.0287 - val_mae: 2.0287\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3119 - mae: 1.3119 - val_loss: 2.0467 - val_mae: 2.0467\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2930 - mae: 1.2930 - val_loss: 2.0285 - val_mae: 2.0285\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3068 - mae: 1.3068 - val_loss: 2.0302 - val_mae: 2.0302\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2919 - mae: 1.2919 - val_loss: 2.0618 - val_mae: 2.0618\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2614 - mae: 1.2614 - val_loss: 2.0364 - val_mae: 2.0364\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3149 - mae: 1.3149 - val_loss: 2.0887 - val_mae: 2.0887\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3141 - mae: 1.3141 - val_loss: 2.0435 - val_mae: 2.0435\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2996 - mae: 1.2996 - val_loss: 2.0819 - val_mae: 2.0819\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3805 - mae: 1.3805 - val_loss: 2.1526 - val_mae: 2.1526\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3869 - mae: 1.3869 - val_loss: 2.0944 - val_mae: 2.0944\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3436 - mae: 1.3436 - val_loss: 2.0328 - val_mae: 2.0328\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3722 - mae: 1.3722 - val_loss: 2.0220 - val_mae: 2.0220\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3417 - mae: 1.3417 - val_loss: 2.0444 - val_mae: 2.0444\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2904 - mae: 1.2904 - val_loss: 2.0120 - val_mae: 2.0120\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2561 - mae: 1.2561 - val_loss: 2.0206 - val_mae: 2.0206\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3559 - mae: 1.3559 - val_loss: 2.0313 - val_mae: 2.0313\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3006 - mae: 1.3006 - val_loss: 2.0311 - val_mae: 2.0311\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2738 - mae: 1.2738 - val_loss: 2.0036 - val_mae: 2.0036\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2314 - mae: 1.2314 - val_loss: 2.0081 - val_mae: 2.0081\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2122 - mae: 1.2122 - val_loss: 2.0071 - val_mae: 2.0071\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2180 - mae: 1.2180 - val_loss: 2.0285 - val_mae: 2.0285\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2085 - mae: 1.2085 - val_loss: 1.9769 - val_mae: 1.9769\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2069 - mae: 1.2069 - val_loss: 1.9969 - val_mae: 1.9969\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2310 - mae: 1.2310 - val_loss: 2.0322 - val_mae: 2.0322\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1974 - mae: 1.1974 - val_loss: 2.0574 - val_mae: 2.0574\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2010 - mae: 1.2010 - val_loss: 1.9905 - val_mae: 1.9905\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1905 - mae: 1.1905 - val_loss: 2.0391 - val_mae: 2.0391\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2038 - mae: 1.2038 - val_loss: 2.0401 - val_mae: 2.0401\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2386 - mae: 1.2386 - val_loss: 2.0314 - val_mae: 2.0314\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1926 - mae: 1.1926 - val_loss: 2.0246 - val_mae: 2.0246\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2219 - mae: 1.2219 - val_loss: 1.9804 - val_mae: 1.9804\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1936 - mae: 1.1936 - val_loss: 1.9970 - val_mae: 1.9970\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1834 - mae: 1.1834 - val_loss: 1.9879 - val_mae: 1.9879\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1656 - mae: 1.1656 - val_loss: 1.9733 - val_mae: 1.9733\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2042 - mae: 1.2042 - val_loss: 2.0171 - val_mae: 2.0171\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2060 - mae: 1.2060 - val_loss: 1.9883 - val_mae: 1.9883\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1987 - mae: 1.1987 - val_loss: 2.0120 - val_mae: 2.0120\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2356 - mae: 1.2356 - val_loss: 2.0075 - val_mae: 2.0075\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2727 - mae: 1.2727 - val_loss: 2.0341 - val_mae: 2.0341\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2024 - mae: 1.2024 - val_loss: 1.9796 - val_mae: 1.9796\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1771 - mae: 1.1771 - val_loss: 2.0029 - val_mae: 2.0029\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1771 - mae: 1.1771 - val_loss: 1.9979 - val_mae: 1.9979\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1917 - mae: 1.1917 - val_loss: 1.9789 - val_mae: 1.9789\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1568 - mae: 1.1568 - val_loss: 1.9596 - val_mae: 1.9596\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1792 - mae: 1.1792 - val_loss: 1.9601 - val_mae: 1.9601\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1768 - mae: 1.1768 - val_loss: 1.9945 - val_mae: 1.9945\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2169 - mae: 1.2169 - val_loss: 2.0032 - val_mae: 2.0032\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2041 - mae: 1.2041 - val_loss: 2.0192 - val_mae: 2.0192\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1360 - mae: 1.1360 - val_loss: 2.0060 - val_mae: 2.0060\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1566 - mae: 1.1566 - val_loss: 1.9681 - val_mae: 1.9681\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1552 - mae: 1.1552 - val_loss: 1.9740 - val_mae: 1.9740\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2013 - mae: 1.2013 - val_loss: 2.0196 - val_mae: 2.0196\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1801 - mae: 1.1801 - val_loss: 1.9813 - val_mae: 1.9813\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1654 - mae: 1.1654 - val_loss: 1.9792 - val_mae: 1.9792\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1261 - mae: 1.1261 - val_loss: 1.9562 - val_mae: 1.9562\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1981 - mae: 1.1981 - val_loss: 1.9645 - val_mae: 1.9645\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1454 - mae: 1.1454 - val_loss: 1.9913 - val_mae: 1.9913\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1628 - mae: 1.1628 - val_loss: 1.9629 - val_mae: 1.9629\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1285 - mae: 1.1285 - val_loss: 1.9920 - val_mae: 1.9920\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1328 - mae: 1.1328 - val_loss: 1.9837 - val_mae: 1.9837\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2248 - mae: 1.2248 - val_loss: 1.9598 - val_mae: 1.9598\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1561 - mae: 1.1561 - val_loss: 1.9945 - val_mae: 1.9945\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1095 - mae: 1.1095 - val_loss: 1.9614 - val_mae: 1.9614\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1372 - mae: 1.1372 - val_loss: 1.9527 - val_mae: 1.9527\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1032 - mae: 1.1032 - val_loss: 1.9434 - val_mae: 1.9434\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1200 - mae: 1.1200 - val_loss: 1.9507 - val_mae: 1.9507\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1603 - mae: 1.1603 - val_loss: 1.9865 - val_mae: 1.9865\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0994 - mae: 1.0994 - val_loss: 1.9664 - val_mae: 1.9664\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1541 - mae: 1.1541 - val_loss: 1.9930 - val_mae: 1.9930\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1031 - mae: 1.1031 - val_loss: 1.9447 - val_mae: 1.9447\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0989 - mae: 1.0989 - val_loss: 1.9603 - val_mae: 1.9603\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1183 - mae: 1.1183 - val_loss: 1.9658 - val_mae: 1.9658\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1103 - mae: 1.1103 - val_loss: 1.9354 - val_mae: 1.9354\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0741 - mae: 1.0741 - val_loss: 1.9437 - val_mae: 1.9437\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1189 - mae: 1.1189 - val_loss: 1.9279 - val_mae: 1.9279\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1259 - mae: 1.1259 - val_loss: 1.9458 - val_mae: 1.9458\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1374 - mae: 1.1374 - val_loss: 1.8873 - val_mae: 1.8873\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1536 - mae: 1.1536 - val_loss: 1.9424 - val_mae: 1.9424\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1164 - mae: 1.1164 - val_loss: 1.8813 - val_mae: 1.8813\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0822 - mae: 1.0822 - val_loss: 1.9163 - val_mae: 1.9163\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0840 - mae: 1.0840 - val_loss: 1.9168 - val_mae: 1.9168\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1154 - mae: 1.1154 - val_loss: 1.8892 - val_mae: 1.8892\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0917 - mae: 1.0917 - val_loss: 1.9120 - val_mae: 1.9120\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1099 - mae: 1.1099 - val_loss: 1.9153 - val_mae: 1.9153\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1017 - mae: 1.1017 - val_loss: 1.8893 - val_mae: 1.8893\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1153 - mae: 1.1153 - val_loss: 1.9042 - val_mae: 1.9042\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1370 - mae: 1.1370 - val_loss: 1.8893 - val_mae: 1.8893\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0958 - mae: 1.0958 - val_loss: 1.8957 - val_mae: 1.8957\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0828 - mae: 1.0828 - val_loss: 1.8664 - val_mae: 1.8664\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0573 - mae: 1.0573 - val_loss: 1.9034 - val_mae: 1.9034\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0734 - mae: 1.0734 - val_loss: 1.8781 - val_mae: 1.8781\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1454 - mae: 1.1454 - val_loss: 1.8386 - val_mae: 1.8386\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1371 - mae: 1.1371 - val_loss: 1.8506 - val_mae: 1.8506\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0638 - mae: 1.0638 - val_loss: 1.8501 - val_mae: 1.8501\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0643 - mae: 1.0643 - val_loss: 1.8543 - val_mae: 1.8543\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0675 - mae: 1.0675 - val_loss: 1.8305 - val_mae: 1.8305\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0690 - mae: 1.0690 - val_loss: 1.8749 - val_mae: 1.8749\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0613 - mae: 1.0613 - val_loss: 1.8542 - val_mae: 1.8542\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0627 - mae: 1.0627 - val_loss: 1.8724 - val_mae: 1.8724\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0287 - mae: 1.0287 - val_loss: 1.8596 - val_mae: 1.8596\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0486 - mae: 1.0486 - val_loss: 1.8593 - val_mae: 1.8593\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9890 - mae: 0.9890 - val_loss: 1.8453 - val_mae: 1.8453\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0131 - mae: 1.0131 - val_loss: 1.8596 - val_mae: 1.8596\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9983 - mae: 0.9983 - val_loss: 1.8395 - val_mae: 1.8395\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0013 - mae: 1.0013 - val_loss: 1.8619 - val_mae: 1.8619\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0039 - mae: 1.0039 - val_loss: 1.8892 - val_mae: 1.8892\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0142 - mae: 1.0142 - val_loss: 1.8597 - val_mae: 1.8597\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0074 - mae: 1.0074 - val_loss: 1.8341 - val_mae: 1.8341\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9826 - mae: 0.9826 - val_loss: 1.8796 - val_mae: 1.8796\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0299 - mae: 1.0299 - val_loss: 1.8742 - val_mae: 1.8742\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0129 - mae: 1.0129 - val_loss: 1.8382 - val_mae: 1.8382\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9905 - mae: 0.9905 - val_loss: 1.8116 - val_mae: 1.8116\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e52d57ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "de55a7c8-3997-428c-e0b1-1c65e4ed3b2f"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "errors = predictions - test_labels\n",
        "\n",
        "mae = sum(abs(predictions - test_labels)) / len(predictions) # Enter your code here\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae21cbe1-deff-4911-9099-9ef65d402099"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 16ms/step - loss: 9.4148 - mean_squared_logarithmic_error: 9.4148 - val_loss: 8.3335 - val_mean_squared_logarithmic_error: 8.3335\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3288 - mean_squared_logarithmic_error: 7.3288 - val_loss: 5.8143 - val_mean_squared_logarithmic_error: 5.8143\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1740 - mean_squared_logarithmic_error: 5.1740 - val_loss: 3.9850 - val_mean_squared_logarithmic_error: 3.9850\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.4425 - mean_squared_logarithmic_error: 3.4425 - val_loss: 2.4799 - val_mean_squared_logarithmic_error: 2.4799\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1381 - mean_squared_logarithmic_error: 2.1381 - val_loss: 1.4967 - val_mean_squared_logarithmic_error: 1.4967\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3173 - mean_squared_logarithmic_error: 1.3173 - val_loss: 0.9015 - val_mean_squared_logarithmic_error: 0.9015\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.8216 - mean_squared_logarithmic_error: 0.8216 - val_loss: 0.5520 - val_mean_squared_logarithmic_error: 0.5520\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.5306 - mean_squared_logarithmic_error: 0.5306 - val_loss: 0.3528 - val_mean_squared_logarithmic_error: 0.3528\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.3595 - mean_squared_logarithmic_error: 0.3595 - val_loss: 0.2413 - val_mean_squared_logarithmic_error: 0.2413\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2623 - mean_squared_logarithmic_error: 0.2623 - val_loss: 0.1745 - val_mean_squared_logarithmic_error: 0.1745\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.2010 - mean_squared_logarithmic_error: 0.2010 - val_loss: 0.1350 - val_mean_squared_logarithmic_error: 0.1350\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1642 - mean_squared_logarithmic_error: 0.1642 - val_loss: 0.1108 - val_mean_squared_logarithmic_error: 0.1108\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1391 - mean_squared_logarithmic_error: 0.1391 - val_loss: 0.0959 - val_mean_squared_logarithmic_error: 0.0959\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1222 - mean_squared_logarithmic_error: 0.1222 - val_loss: 0.0856 - val_mean_squared_logarithmic_error: 0.0856\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1109 - mean_squared_logarithmic_error: 0.1109 - val_loss: 0.0778 - val_mean_squared_logarithmic_error: 0.0778\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1013 - mean_squared_logarithmic_error: 0.1013 - val_loss: 0.0721 - val_mean_squared_logarithmic_error: 0.0721\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0936 - mean_squared_logarithmic_error: 0.0936 - val_loss: 0.0674 - val_mean_squared_logarithmic_error: 0.0674\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0875 - mean_squared_logarithmic_error: 0.0875 - val_loss: 0.0637 - val_mean_squared_logarithmic_error: 0.0637\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0819 - mean_squared_logarithmic_error: 0.0819 - val_loss: 0.0600 - val_mean_squared_logarithmic_error: 0.0600\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0768 - mean_squared_logarithmic_error: 0.0768 - val_loss: 0.0570 - val_mean_squared_logarithmic_error: 0.0570\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0726 - mean_squared_logarithmic_error: 0.0726 - val_loss: 0.0540 - val_mean_squared_logarithmic_error: 0.0540\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0683 - mean_squared_logarithmic_error: 0.0683 - val_loss: 0.0514 - val_mean_squared_logarithmic_error: 0.0514\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0645 - mean_squared_logarithmic_error: 0.0645 - val_loss: 0.0491 - val_mean_squared_logarithmic_error: 0.0491\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0612 - mean_squared_logarithmic_error: 0.0612 - val_loss: 0.0470 - val_mean_squared_logarithmic_error: 0.0470\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0580 - mean_squared_logarithmic_error: 0.0580 - val_loss: 0.0452 - val_mean_squared_logarithmic_error: 0.0452\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0552 - mean_squared_logarithmic_error: 0.0552 - val_loss: 0.0434 - val_mean_squared_logarithmic_error: 0.0434\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0527 - mean_squared_logarithmic_error: 0.0527 - val_loss: 0.0418 - val_mean_squared_logarithmic_error: 0.0418\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0502 - mean_squared_logarithmic_error: 0.0502 - val_loss: 0.0402 - val_mean_squared_logarithmic_error: 0.0402\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0479 - mean_squared_logarithmic_error: 0.0479 - val_loss: 0.0388 - val_mean_squared_logarithmic_error: 0.0388\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0458 - mean_squared_logarithmic_error: 0.0458 - val_loss: 0.0377 - val_mean_squared_logarithmic_error: 0.0377\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0438 - mean_squared_logarithmic_error: 0.0438 - val_loss: 0.0365 - val_mean_squared_logarithmic_error: 0.0365\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0423 - mean_squared_logarithmic_error: 0.0423 - val_loss: 0.0354 - val_mean_squared_logarithmic_error: 0.0354\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0405 - mean_squared_logarithmic_error: 0.0405 - val_loss: 0.0347 - val_mean_squared_logarithmic_error: 0.0347\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0389 - mean_squared_logarithmic_error: 0.0389 - val_loss: 0.0339 - val_mean_squared_logarithmic_error: 0.0339\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0376 - mean_squared_logarithmic_error: 0.0376 - val_loss: 0.0332 - val_mean_squared_logarithmic_error: 0.0332\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0364 - mean_squared_logarithmic_error: 0.0364 - val_loss: 0.0329 - val_mean_squared_logarithmic_error: 0.0329\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0354 - mean_squared_logarithmic_error: 0.0354 - val_loss: 0.0323 - val_mean_squared_logarithmic_error: 0.0323\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0345 - mean_squared_logarithmic_error: 0.0345 - val_loss: 0.0316 - val_mean_squared_logarithmic_error: 0.0316\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0336 - mean_squared_logarithmic_error: 0.0336 - val_loss: 0.0311 - val_mean_squared_logarithmic_error: 0.0311\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0327 - mean_squared_logarithmic_error: 0.0327 - val_loss: 0.0308 - val_mean_squared_logarithmic_error: 0.0308\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0320 - mean_squared_logarithmic_error: 0.0320 - val_loss: 0.0307 - val_mean_squared_logarithmic_error: 0.0307\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0314 - mean_squared_logarithmic_error: 0.0314 - val_loss: 0.0302 - val_mean_squared_logarithmic_error: 0.0302\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0307 - mean_squared_logarithmic_error: 0.0307 - val_loss: 0.0298 - val_mean_squared_logarithmic_error: 0.0298\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0301 - mean_squared_logarithmic_error: 0.0301 - val_loss: 0.0296 - val_mean_squared_logarithmic_error: 0.0296\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0296 - mean_squared_logarithmic_error: 0.0296 - val_loss: 0.0290 - val_mean_squared_logarithmic_error: 0.0290\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0291 - mean_squared_logarithmic_error: 0.0291 - val_loss: 0.0286 - val_mean_squared_logarithmic_error: 0.0286\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - mean_squared_logarithmic_error: 0.0286 - val_loss: 0.0286 - val_mean_squared_logarithmic_error: 0.0286\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0283 - mean_squared_logarithmic_error: 0.0283 - val_loss: 0.0282 - val_mean_squared_logarithmic_error: 0.0282\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0278 - mean_squared_logarithmic_error: 0.0278 - val_loss: 0.0282 - val_mean_squared_logarithmic_error: 0.0282\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0273 - mean_squared_logarithmic_error: 0.0273 - val_loss: 0.0276 - val_mean_squared_logarithmic_error: 0.0276\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0270 - mean_squared_logarithmic_error: 0.0270 - val_loss: 0.0275 - val_mean_squared_logarithmic_error: 0.0275\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0265 - mean_squared_logarithmic_error: 0.0265 - val_loss: 0.0274 - val_mean_squared_logarithmic_error: 0.0274\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0263 - mean_squared_logarithmic_error: 0.0263 - val_loss: 0.0271 - val_mean_squared_logarithmic_error: 0.0271\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0259 - mean_squared_logarithmic_error: 0.0259 - val_loss: 0.0264 - val_mean_squared_logarithmic_error: 0.0264\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - mean_squared_logarithmic_error: 0.0256 - val_loss: 0.0267 - val_mean_squared_logarithmic_error: 0.0267\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0253 - mean_squared_logarithmic_error: 0.0253 - val_loss: 0.0264 - val_mean_squared_logarithmic_error: 0.0264\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - mean_squared_logarithmic_error: 0.0249 - val_loss: 0.0262 - val_mean_squared_logarithmic_error: 0.0262\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0247 - mean_squared_logarithmic_error: 0.0247 - val_loss: 0.0260 - val_mean_squared_logarithmic_error: 0.0260\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - mean_squared_logarithmic_error: 0.0245 - val_loss: 0.0252 - val_mean_squared_logarithmic_error: 0.0252\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0243 - mean_squared_logarithmic_error: 0.0243 - val_loss: 0.0258 - val_mean_squared_logarithmic_error: 0.0258\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - mean_squared_logarithmic_error: 0.0238 - val_loss: 0.0251 - val_mean_squared_logarithmic_error: 0.0251\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - mean_squared_logarithmic_error: 0.0238 - val_loss: 0.0247 - val_mean_squared_logarithmic_error: 0.0247\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - mean_squared_logarithmic_error: 0.0235 - val_loss: 0.0248 - val_mean_squared_logarithmic_error: 0.0248\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0232 - mean_squared_logarithmic_error: 0.0232 - val_loss: 0.0247 - val_mean_squared_logarithmic_error: 0.0247\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - mean_squared_logarithmic_error: 0.0231 - val_loss: 0.0248 - val_mean_squared_logarithmic_error: 0.0248\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0228 - mean_squared_logarithmic_error: 0.0228 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0223 - mean_squared_logarithmic_error: 0.0223 - val_loss: 0.0241 - val_mean_squared_logarithmic_error: 0.0241\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - mean_squared_logarithmic_error: 0.0223 - val_loss: 0.0237 - val_mean_squared_logarithmic_error: 0.0237\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0233 - val_mean_squared_logarithmic_error: 0.0233\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - mean_squared_logarithmic_error: 0.0219 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - mean_squared_logarithmic_error: 0.0217 - val_loss: 0.0231 - val_mean_squared_logarithmic_error: 0.0231\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0216 - mean_squared_logarithmic_error: 0.0216 - val_loss: 0.0224 - val_mean_squared_logarithmic_error: 0.0224\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - mean_squared_logarithmic_error: 0.0215 - val_loss: 0.0227 - val_mean_squared_logarithmic_error: 0.0227\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0213 - mean_squared_logarithmic_error: 0.0213 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0226 - val_mean_squared_logarithmic_error: 0.0226\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0219 - val_mean_squared_logarithmic_error: 0.0219\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - mean_squared_logarithmic_error: 0.0209 - val_loss: 0.0223 - val_mean_squared_logarithmic_error: 0.0223\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - mean_squared_logarithmic_error: 0.0209 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0217 - val_mean_squared_logarithmic_error: 0.0217\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - mean_squared_logarithmic_error: 0.0206 - val_loss: 0.0221 - val_mean_squared_logarithmic_error: 0.0221\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0196 - mean_squared_logarithmic_error: 0.0196 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0209 - val_mean_squared_logarithmic_error: 0.0209\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - mean_squared_logarithmic_error: 0.0190 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0209 - val_mean_squared_logarithmic_error: 0.0209\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - mean_squared_logarithmic_error: 0.0176 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0155 - mean_squared_logarithmic_error: 0.0155 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0156 - mean_squared_logarithmic_error: 0.0156 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0157 - mean_squared_logarithmic_error: 0.0157 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0153 - mean_squared_logarithmic_error: 0.0153 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e527e29d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "4a326055-5849-45db-800c-c05a97b00a06"
      },
      "source": [
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "#estimated_outputs = np.zeros(n)\n",
        "\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "actual_outputs = predictions - test_labels\n",
        "\n",
        "msle = sum((np.log(predictions + 1) - np.log(test_labels + 1)) ** 2) / len(predictions) # Enter your code here\n",
        "\n",
        "plt.plot(actual_outputs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n",
        "\n",
        "Because if an output was 0, then we would be asking for the log of 0, which is undefined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n",
        "\n",
        "I find it interesting that there are so many options to complete various tasks. It seems that as we go down from MSE,MAE, and then to MSLE, the graphed lines become thinner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE.\n",
        "\n",
        "The model using MAPE performs the worst in terms of val_loss when compared to the results of models that use the other loss functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe2a09f-8148-4d52-86c5-bb1c4c6f57e3"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= \"MAPE\",\n",
        "              metrics=[\"MAPE\"])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 20ms/step - loss: 95.5257 - MAPE: 95.5257 - val_loss: 90.9833 - val_MAPE: 90.9833\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 85.4712 - MAPE: 85.4712 - val_loss: 79.2484 - val_MAPE: 79.2484\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 72.4428 - MAPE: 72.4428 - val_loss: 65.7644 - val_MAPE: 65.7644\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 57.9915 - MAPE: 57.9915 - val_loss: 50.6684 - val_MAPE: 50.6684\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 42.0800 - MAPE: 42.0800 - val_loss: 33.8061 - val_MAPE: 33.8061\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 31.2330 - MAPE: 31.2330 - val_loss: 26.6593 - val_MAPE: 26.6593\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26.0506 - MAPE: 26.0506 - val_loss: 20.6044 - val_MAPE: 20.6044\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 21.4452 - MAPE: 21.4452 - val_loss: 17.5725 - val_MAPE: 17.5725\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.1065 - MAPE: 18.1065 - val_loss: 16.8846 - val_MAPE: 16.8846\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.2664 - MAPE: 16.2664 - val_loss: 16.4433 - val_MAPE: 16.4433\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.0960 - MAPE: 15.0960 - val_loss: 15.7525 - val_MAPE: 15.7525\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.4330 - MAPE: 14.4330 - val_loss: 15.1335 - val_MAPE: 15.1335\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.1964 - MAPE: 14.1964 - val_loss: 14.7046 - val_MAPE: 14.7046\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.5116 - MAPE: 13.5116 - val_loss: 14.1654 - val_MAPE: 14.1654\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.8711 - MAPE: 12.8711 - val_loss: 13.6809 - val_MAPE: 13.6809\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.5850 - MAPE: 12.5850 - val_loss: 13.7682 - val_MAPE: 13.7682\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.9472 - MAPE: 11.9472 - val_loss: 13.2217 - val_MAPE: 13.2217\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7153 - MAPE: 11.7153 - val_loss: 13.5327 - val_MAPE: 13.5327\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.2684 - MAPE: 11.2684 - val_loss: 12.9358 - val_MAPE: 12.9358\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.8918 - MAPE: 10.8918 - val_loss: 12.8695 - val_MAPE: 12.8695\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7270 - MAPE: 10.7270 - val_loss: 12.5837 - val_MAPE: 12.5837\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5659 - MAPE: 10.5659 - val_loss: 12.3013 - val_MAPE: 12.3013\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2877 - MAPE: 10.2877 - val_loss: 12.5529 - val_MAPE: 12.5529\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1922 - MAPE: 10.1922 - val_loss: 11.9688 - val_MAPE: 11.9688\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2710 - MAPE: 10.2710 - val_loss: 12.4011 - val_MAPE: 12.4011\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.0430 - MAPE: 10.0430 - val_loss: 12.1430 - val_MAPE: 12.1430\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1325 - MAPE: 10.1325 - val_loss: 12.0289 - val_MAPE: 12.0289\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9794 - MAPE: 9.9794 - val_loss: 11.9797 - val_MAPE: 11.9797\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8323 - MAPE: 9.8323 - val_loss: 12.1710 - val_MAPE: 12.1710\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4877 - MAPE: 9.4877 - val_loss: 11.6249 - val_MAPE: 11.6249\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4557 - MAPE: 9.4557 - val_loss: 12.1003 - val_MAPE: 12.1003\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.5981 - MAPE: 9.5981 - val_loss: 11.5246 - val_MAPE: 11.5246\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.8625 - MAPE: 9.8625 - val_loss: 12.0952 - val_MAPE: 12.0952\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4266 - MAPE: 9.4266 - val_loss: 11.9625 - val_MAPE: 11.9625\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.4951 - MAPE: 9.4951 - val_loss: 11.7711 - val_MAPE: 11.7711\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3263 - MAPE: 9.3263 - val_loss: 12.1106 - val_MAPE: 12.1106\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2786 - MAPE: 9.2786 - val_loss: 11.6058 - val_MAPE: 11.6058\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0104 - MAPE: 9.0104 - val_loss: 11.3858 - val_MAPE: 11.3858\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0733 - MAPE: 9.0733 - val_loss: 11.7729 - val_MAPE: 11.7729\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1426 - MAPE: 9.1426 - val_loss: 11.5896 - val_MAPE: 11.5896\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0518 - MAPE: 9.0518 - val_loss: 11.3702 - val_MAPE: 11.3702\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9365 - MAPE: 8.9365 - val_loss: 11.1397 - val_MAPE: 11.1397\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6566 - MAPE: 9.6566 - val_loss: 11.5852 - val_MAPE: 11.5852\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7670 - MAPE: 8.7670 - val_loss: 11.4206 - val_MAPE: 11.4206\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0453 - MAPE: 9.0453 - val_loss: 10.5246 - val_MAPE: 10.5246\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1451 - MAPE: 9.1451 - val_loss: 11.4015 - val_MAPE: 11.4015\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0723 - MAPE: 9.0723 - val_loss: 11.2216 - val_MAPE: 11.2216\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6147 - MAPE: 8.6147 - val_loss: 10.6174 - val_MAPE: 10.6174\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4862 - MAPE: 8.4862 - val_loss: 11.3660 - val_MAPE: 11.3660\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6937 - MAPE: 8.6937 - val_loss: 11.4015 - val_MAPE: 11.4015\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7521 - MAPE: 8.7521 - val_loss: 11.4369 - val_MAPE: 11.4369\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4047 - MAPE: 8.4047 - val_loss: 11.0932 - val_MAPE: 11.0932\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2685 - MAPE: 8.2685 - val_loss: 10.7108 - val_MAPE: 10.7108\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1542 - MAPE: 8.1542 - val_loss: 11.0636 - val_MAPE: 11.0636\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1315 - MAPE: 8.1315 - val_loss: 10.9350 - val_MAPE: 10.9350\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1215 - MAPE: 8.1215 - val_loss: 11.1140 - val_MAPE: 11.1140\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9881 - MAPE: 7.9881 - val_loss: 10.6996 - val_MAPE: 10.6996\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1368 - MAPE: 8.1368 - val_loss: 11.3982 - val_MAPE: 11.3982\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0713 - MAPE: 8.0713 - val_loss: 10.7337 - val_MAPE: 10.7337\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9908 - MAPE: 7.9908 - val_loss: 10.8403 - val_MAPE: 10.8403\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0030 - MAPE: 8.0030 - val_loss: 10.3523 - val_MAPE: 10.3523\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9279 - MAPE: 7.9279 - val_loss: 11.1995 - val_MAPE: 11.1995\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8933 - MAPE: 7.8933 - val_loss: 10.9939 - val_MAPE: 10.9939\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8165 - MAPE: 7.8165 - val_loss: 10.4612 - val_MAPE: 10.4612\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7826 - MAPE: 7.7826 - val_loss: 10.8595 - val_MAPE: 10.8595\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7375 - MAPE: 7.7375 - val_loss: 11.1962 - val_MAPE: 11.1962\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7365 - MAPE: 7.7365 - val_loss: 10.8349 - val_MAPE: 10.8349\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6505 - MAPE: 7.6505 - val_loss: 10.6924 - val_MAPE: 10.6924\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5500 - MAPE: 7.5500 - val_loss: 11.5249 - val_MAPE: 11.5249\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8704 - MAPE: 7.8704 - val_loss: 10.4780 - val_MAPE: 10.4780\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8605 - MAPE: 7.8605 - val_loss: 10.2644 - val_MAPE: 10.2644\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7193 - MAPE: 7.7193 - val_loss: 10.8612 - val_MAPE: 10.8612\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6906 - MAPE: 7.6906 - val_loss: 10.7652 - val_MAPE: 10.7652\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6131 - MAPE: 7.6131 - val_loss: 9.9581 - val_MAPE: 9.9581\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8980 - MAPE: 7.8980 - val_loss: 10.9418 - val_MAPE: 10.9418\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9761 - MAPE: 7.9761 - val_loss: 11.1447 - val_MAPE: 11.1447\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7832 - MAPE: 7.7832 - val_loss: 11.0601 - val_MAPE: 11.0601\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6168 - MAPE: 7.6168 - val_loss: 10.9640 - val_MAPE: 10.9640\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4627 - MAPE: 7.4627 - val_loss: 10.7244 - val_MAPE: 10.7244\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3622 - MAPE: 7.3622 - val_loss: 10.2016 - val_MAPE: 10.2016\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1550 - MAPE: 7.1550 - val_loss: 10.2492 - val_MAPE: 10.2492\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2380 - MAPE: 7.2380 - val_loss: 11.0301 - val_MAPE: 11.0301\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5532 - MAPE: 7.5532 - val_loss: 10.5303 - val_MAPE: 10.5303\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2320 - MAPE: 7.2320 - val_loss: 10.6756 - val_MAPE: 10.6756\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5275 - MAPE: 7.5275 - val_loss: 10.4283 - val_MAPE: 10.4283\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 7.3527 - MAPE: 7.3527 - val_loss: 10.5671 - val_MAPE: 10.5671\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.1335 - MAPE: 7.1335 - val_loss: 10.9708 - val_MAPE: 10.9708\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4324 - MAPE: 7.4324 - val_loss: 10.3685 - val_MAPE: 10.3685\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1844 - MAPE: 7.1844 - val_loss: 10.7106 - val_MAPE: 10.7106\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3798 - MAPE: 7.3798 - val_loss: 10.6925 - val_MAPE: 10.6925\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1751 - MAPE: 7.1751 - val_loss: 10.0618 - val_MAPE: 10.0618\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4137 - MAPE: 7.4137 - val_loss: 10.5626 - val_MAPE: 10.5626\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9775 - MAPE: 6.9775 - val_loss: 10.6834 - val_MAPE: 10.6834\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0120 - MAPE: 7.0120 - val_loss: 9.8938 - val_MAPE: 9.8938\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1652 - MAPE: 7.1652 - val_loss: 11.0073 - val_MAPE: 11.0073\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8805 - MAPE: 6.8805 - val_loss: 10.5799 - val_MAPE: 10.5799\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2038 - MAPE: 7.2038 - val_loss: 10.1313 - val_MAPE: 10.1313\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8960 - MAPE: 6.8960 - val_loss: 10.8773 - val_MAPE: 10.8773\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2495 - MAPE: 7.2495 - val_loss: 10.7463 - val_MAPE: 10.7463\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1352 - MAPE: 7.1352 - val_loss: 10.5022 - val_MAPE: 10.5022\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4614 - MAPE: 7.4614 - val_loss: 10.8438 - val_MAPE: 10.8438\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3544 - MAPE: 7.3544 - val_loss: 10.6285 - val_MAPE: 10.6285\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9276 - MAPE: 6.9276 - val_loss: 10.5712 - val_MAPE: 10.5712\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8406 - MAPE: 6.8406 - val_loss: 10.5499 - val_MAPE: 10.5499\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9981 - MAPE: 6.9981 - val_loss: 11.1898 - val_MAPE: 11.1898\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0688 - MAPE: 7.0688 - val_loss: 10.9820 - val_MAPE: 10.9820\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8964 - MAPE: 6.8964 - val_loss: 10.1038 - val_MAPE: 10.1038\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9371 - MAPE: 6.9371 - val_loss: 10.5513 - val_MAPE: 10.5513\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7781 - MAPE: 6.7781 - val_loss: 10.6365 - val_MAPE: 10.6365\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6533 - MAPE: 6.6533 - val_loss: 10.2987 - val_MAPE: 10.2987\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0203 - MAPE: 7.0203 - val_loss: 11.1369 - val_MAPE: 11.1369\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7095 - MAPE: 6.7095 - val_loss: 10.7417 - val_MAPE: 10.7417\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8397 - MAPE: 6.8397 - val_loss: 10.5059 - val_MAPE: 10.5059\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7426 - MAPE: 6.7426 - val_loss: 10.9403 - val_MAPE: 10.9403\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5240 - MAPE: 6.5240 - val_loss: 10.4780 - val_MAPE: 10.4780\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5440 - MAPE: 6.5440 - val_loss: 10.3305 - val_MAPE: 10.3305\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5612 - MAPE: 6.5612 - val_loss: 10.5072 - val_MAPE: 10.5072\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7029 - MAPE: 6.7029 - val_loss: 11.0650 - val_MAPE: 11.0650\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5391 - MAPE: 6.5391 - val_loss: 10.3884 - val_MAPE: 10.3884\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5931 - MAPE: 6.5931 - val_loss: 10.3923 - val_MAPE: 10.3923\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4046 - MAPE: 6.4046 - val_loss: 10.5396 - val_MAPE: 10.5396\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3949 - MAPE: 6.3949 - val_loss: 10.6774 - val_MAPE: 10.6774\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3354 - MAPE: 6.3354 - val_loss: 10.6411 - val_MAPE: 10.6411\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2667 - MAPE: 6.2667 - val_loss: 10.8228 - val_MAPE: 10.8228\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5020 - MAPE: 6.5020 - val_loss: 10.3935 - val_MAPE: 10.3935\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2464 - MAPE: 6.2464 - val_loss: 10.8106 - val_MAPE: 10.8106\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2802 - MAPE: 6.2802 - val_loss: 11.0832 - val_MAPE: 11.0832\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4559 - MAPE: 6.4559 - val_loss: 10.5189 - val_MAPE: 10.5189\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4690 - MAPE: 6.4690 - val_loss: 10.6676 - val_MAPE: 10.6676\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6576 - MAPE: 6.6576 - val_loss: 9.9347 - val_MAPE: 9.9347\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3303 - MAPE: 6.3303 - val_loss: 10.9572 - val_MAPE: 10.9572\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3580 - MAPE: 6.3580 - val_loss: 10.5229 - val_MAPE: 10.5229\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.2730 - MAPE: 6.2730 - val_loss: 10.2171 - val_MAPE: 10.2171\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3114 - MAPE: 6.3114 - val_loss: 10.4137 - val_MAPE: 10.4137\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6972 - MAPE: 6.6972 - val_loss: 10.6474 - val_MAPE: 10.6474\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2872 - MAPE: 6.2872 - val_loss: 10.4682 - val_MAPE: 10.4682\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7416 - MAPE: 6.7416 - val_loss: 10.4737 - val_MAPE: 10.4737\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3498 - MAPE: 6.3498 - val_loss: 10.7405 - val_MAPE: 10.7405\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0718 - MAPE: 6.0718 - val_loss: 10.5694 - val_MAPE: 10.5694\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1421 - MAPE: 6.1421 - val_loss: 10.5977 - val_MAPE: 10.5977\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3245 - MAPE: 6.3245 - val_loss: 10.5303 - val_MAPE: 10.5303\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8302 - MAPE: 6.8302 - val_loss: 10.3697 - val_MAPE: 10.3697\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0475 - MAPE: 6.0475 - val_loss: 10.1701 - val_MAPE: 10.1701\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9303 - MAPE: 5.9303 - val_loss: 10.3434 - val_MAPE: 10.3434\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9817 - MAPE: 5.9817 - val_loss: 10.2430 - val_MAPE: 10.2430\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0419 - MAPE: 6.0419 - val_loss: 10.7275 - val_MAPE: 10.7275\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3088 - MAPE: 6.3088 - val_loss: 10.1238 - val_MAPE: 10.1238\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0312 - MAPE: 6.0312 - val_loss: 10.4853 - val_MAPE: 10.4853\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0307 - MAPE: 6.0307 - val_loss: 10.1224 - val_MAPE: 10.1224\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8913 - MAPE: 5.8913 - val_loss: 10.5235 - val_MAPE: 10.5235\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0134 - MAPE: 6.0134 - val_loss: 10.9684 - val_MAPE: 10.9684\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1199 - MAPE: 6.1199 - val_loss: 9.9353 - val_MAPE: 9.9353\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0950 - MAPE: 6.0950 - val_loss: 10.6838 - val_MAPE: 10.6838\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2314 - MAPE: 6.2314 - val_loss: 10.9646 - val_MAPE: 10.9646\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9892 - MAPE: 5.9892 - val_loss: 10.1046 - val_MAPE: 10.1046\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1032 - MAPE: 6.1032 - val_loss: 10.4509 - val_MAPE: 10.4509\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1262 - MAPE: 6.1262 - val_loss: 11.2603 - val_MAPE: 11.2603\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3803 - MAPE: 6.3803 - val_loss: 10.0276 - val_MAPE: 10.0276\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8180 - MAPE: 5.8180 - val_loss: 10.9076 - val_MAPE: 10.9076\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7206 - MAPE: 5.7206 - val_loss: 10.5080 - val_MAPE: 10.5080\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7265 - MAPE: 5.7265 - val_loss: 10.2670 - val_MAPE: 10.2670\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9426 - MAPE: 5.9426 - val_loss: 10.6461 - val_MAPE: 10.6461\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5837 - MAPE: 5.5837 - val_loss: 10.0950 - val_MAPE: 10.0950\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7027 - MAPE: 5.7027 - val_loss: 9.8136 - val_MAPE: 9.8136\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7661 - MAPE: 5.7661 - val_loss: 10.6261 - val_MAPE: 10.6261\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5870 - MAPE: 5.5870 - val_loss: 9.9963 - val_MAPE: 9.9963\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2632 - MAPE: 6.2632 - val_loss: 9.9999 - val_MAPE: 9.9999\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7303 - MAPE: 5.7303 - val_loss: 10.1840 - val_MAPE: 10.1840\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8724 - MAPE: 5.8724 - val_loss: 10.2649 - val_MAPE: 10.2649\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7503 - MAPE: 5.7503 - val_loss: 9.9014 - val_MAPE: 9.9014\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8056 - MAPE: 5.8056 - val_loss: 9.8938 - val_MAPE: 9.8938\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9063 - MAPE: 5.9063 - val_loss: 10.3953 - val_MAPE: 10.3953\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6931 - MAPE: 5.6931 - val_loss: 10.5168 - val_MAPE: 10.5168\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.6336 - MAPE: 5.6336 - val_loss: 10.2644 - val_MAPE: 10.2644\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3835 - MAPE: 5.3835 - val_loss: 10.4543 - val_MAPE: 10.4543\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4130 - MAPE: 5.4130 - val_loss: 10.5904 - val_MAPE: 10.5904\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.9845 - MAPE: 5.9845 - val_loss: 9.9535 - val_MAPE: 9.9535\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1471 - MAPE: 6.1471 - val_loss: 10.3375 - val_MAPE: 10.3375\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.7769 - MAPE: 5.7769 - val_loss: 11.0203 - val_MAPE: 11.0203\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7885 - MAPE: 5.7885 - val_loss: 9.6527 - val_MAPE: 9.6527\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5802 - MAPE: 5.5802 - val_loss: 10.1454 - val_MAPE: 10.1454\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8730 - MAPE: 5.8730 - val_loss: 9.8952 - val_MAPE: 9.8952\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8414 - MAPE: 5.8414 - val_loss: 10.3183 - val_MAPE: 10.3183\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8347 - MAPE: 5.8347 - val_loss: 10.6493 - val_MAPE: 10.6493\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6949 - MAPE: 5.6949 - val_loss: 10.3427 - val_MAPE: 10.3427\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.4555 - MAPE: 5.4555 - val_loss: 10.1862 - val_MAPE: 10.1862\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2427 - MAPE: 5.2427 - val_loss: 10.0130 - val_MAPE: 10.0130\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2964 - MAPE: 5.2964 - val_loss: 10.0399 - val_MAPE: 10.0399\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.4707 - MAPE: 5.4707 - val_loss: 10.5221 - val_MAPE: 10.5221\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3882 - MAPE: 5.3882 - val_loss: 9.7165 - val_MAPE: 9.7165\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2839 - MAPE: 5.2839 - val_loss: 9.7317 - val_MAPE: 9.7317\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0360 - MAPE: 6.0360 - val_loss: 10.1818 - val_MAPE: 10.1818\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3062 - MAPE: 5.3062 - val_loss: 10.3300 - val_MAPE: 10.3300\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3497 - MAPE: 5.3497 - val_loss: 9.6207 - val_MAPE: 9.6207\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3077 - MAPE: 5.3077 - val_loss: 10.1220 - val_MAPE: 10.1220\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0555 - MAPE: 5.0555 - val_loss: 10.3414 - val_MAPE: 10.3414\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5616 - MAPE: 5.5616 - val_loss: 10.3209 - val_MAPE: 10.3209\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.6526 - MAPE: 5.6526 - val_loss: 10.4187 - val_MAPE: 10.4187\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1944 - MAPE: 5.1944 - val_loss: 9.5926 - val_MAPE: 9.5926\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0396 - MAPE: 5.0396 - val_loss: 10.0618 - val_MAPE: 10.0618\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1400 - MAPE: 5.1400 - val_loss: 10.1232 - val_MAPE: 10.1232\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2228 - MAPE: 5.2228 - val_loss: 9.8775 - val_MAPE: 9.8775\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3274 - MAPE: 5.3274 - val_loss: 9.9996 - val_MAPE: 9.9996\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.3429 - MAPE: 5.3429 - val_loss: 10.0345 - val_MAPE: 10.0345\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 5.4767 - MAPE: 5.4767 - val_loss: 10.6426 - val_MAPE: 10.6426\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.5550 - MAPE: 5.5550 - val_loss: 9.6475 - val_MAPE: 9.6475\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3409 - MAPE: 5.3409 - val_loss: 9.2845 - val_MAPE: 9.2845\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1833 - MAPE: 5.1833 - val_loss: 9.8240 - val_MAPE: 9.8240\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1373 - MAPE: 5.1373 - val_loss: 10.2173 - val_MAPE: 10.2173\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.0654 - MAPE: 5.0654 - val_loss: 10.1976 - val_MAPE: 10.1976\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3207 - MAPE: 5.3207 - val_loss: 9.8042 - val_MAPE: 9.8042\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3221 - MAPE: 5.3221 - val_loss: 9.8863 - val_MAPE: 9.8863\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3220 - MAPE: 5.3220 - val_loss: 9.3787 - val_MAPE: 9.3787\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1796 - MAPE: 5.1796 - val_loss: 10.4234 - val_MAPE: 10.4234\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2169 - MAPE: 5.2169 - val_loss: 9.5418 - val_MAPE: 9.5418\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.3068 - MAPE: 5.3068 - val_loss: 10.2872 - val_MAPE: 10.2872\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9959 - MAPE: 4.9959 - val_loss: 9.8391 - val_MAPE: 9.8391\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9565 - MAPE: 4.9565 - val_loss: 10.2155 - val_MAPE: 10.2155\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9682 - MAPE: 4.9682 - val_loss: 10.0400 - val_MAPE: 10.0400\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.1293 - MAPE: 5.1293 - val_loss: 10.2579 - val_MAPE: 10.2579\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2360 - MAPE: 5.2360 - val_loss: 10.0528 - val_MAPE: 10.0528\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.2456 - MAPE: 5.2456 - val_loss: 9.8609 - val_MAPE: 9.8609\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0913 - MAPE: 5.0913 - val_loss: 9.8367 - val_MAPE: 9.8367\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8943 - MAPE: 4.8943 - val_loss: 10.1298 - val_MAPE: 10.1298\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8010 - MAPE: 4.8010 - val_loss: 9.8564 - val_MAPE: 9.8564\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9074 - MAPE: 4.9074 - val_loss: 9.8777 - val_MAPE: 9.8777\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8547 - MAPE: 4.8547 - val_loss: 9.5718 - val_MAPE: 9.5718\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0142 - MAPE: 5.0142 - val_loss: 9.4432 - val_MAPE: 9.4432\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0051 - MAPE: 5.0051 - val_loss: 9.8298 - val_MAPE: 9.8298\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.1411 - MAPE: 5.1411 - val_loss: 9.8674 - val_MAPE: 9.8674\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.2588 - MAPE: 5.2588 - val_loss: 9.7365 - val_MAPE: 9.7365\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9559 - MAPE: 4.9559 - val_loss: 9.6670 - val_MAPE: 9.6670\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.5702 - MAPE: 4.5702 - val_loss: 9.8839 - val_MAPE: 9.8839\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.6768 - MAPE: 4.6768 - val_loss: 9.7328 - val_MAPE: 9.7328\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.5953 - MAPE: 4.5953 - val_loss: 9.7663 - val_MAPE: 9.7663\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.6150 - MAPE: 4.6150 - val_loss: 9.8443 - val_MAPE: 9.8443\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.9392 - MAPE: 4.9392 - val_loss: 10.1969 - val_MAPE: 10.1969\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8973 - MAPE: 4.8973 - val_loss: 9.5379 - val_MAPE: 9.5379\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.7357 - MAPE: 4.7357 - val_loss: 9.8066 - val_MAPE: 9.8066\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.0159 - MAPE: 5.0159 - val_loss: 9.3201 - val_MAPE: 9.3201\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.8245 - MAPE: 4.8245 - val_loss: 9.6259 - val_MAPE: 9.6259\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.4557 - MAPE: 4.4557 - val_loss: 9.7430 - val_MAPE: 9.7430\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.4870 - MAPE: 4.4870 - val_loss: 9.5625 - val_MAPE: 9.5625\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 4.5548 - MAPE: 4.5548 - val_loss: 10.2193 - val_MAPE: 10.2193\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.6796 - MAPE: 4.6796 - val_loss: 10.0881 - val_MAPE: 10.0881\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8789 - MAPE: 4.8789 - val_loss: 9.7875 - val_MAPE: 9.7875\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.9170 - MAPE: 4.9170 - val_loss: 9.4579 - val_MAPE: 9.4579\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4290 - MAPE: 5.4290 - val_loss: 9.9904 - val_MAPE: 9.9904\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.7890 - MAPE: 4.7890 - val_loss: 9.5609 - val_MAPE: 9.5609\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.4793 - MAPE: 4.4793 - val_loss: 10.0522 - val_MAPE: 10.0522\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e53270490>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b76b587-9ad4-4ca1-d43b-0b7c32df4fb3"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 13s 0us/step\n",
            "169017344/169001437 [==============================] - 13s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KDL]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KDL(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74624b98-1bf0-4f50-f7cb-11b5cbc52660"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 17s 5ms/step - loss: 437.4923 - accuracy: 0.0077 - val_loss: 437.4907 - val_accuracy: 0.0100\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0095 - val_loss: 437.4907 - val_accuracy: 0.0050\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0129 - val_loss: 437.4907 - val_accuracy: 0.0499\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4915 - accuracy: 0.0175 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4923 - accuracy: 0.0151 - val_loss: 437.4907 - val_accuracy: 1.0000e-04\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4919 - accuracy: 0.0140 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0121 - val_loss: 437.4907 - val_accuracy: 0.0499\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4916 - accuracy: 0.0086 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0109 - val_loss: 437.4907 - val_accuracy: 3.0000e-04\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0088 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0123 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4918 - accuracy: 0.0071 - val_loss: 437.4907 - val_accuracy: 2.0000e-04\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0115 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0141 - val_loss: 437.4907 - val_accuracy: 0.0499\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4923 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4924 - accuracy: 0.0076 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4925 - accuracy: 0.0165 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0099 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0086 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0151 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0122 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0131 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4924 - accuracy: 0.0099 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0103 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0125 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4924 - accuracy: 0.0100 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0162 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0135 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0159 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0115 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0133 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 437.4920 - accuracy: 0.0120 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 437.4919 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4926 - accuracy: 0.0110 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4923 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0108 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0093 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0102 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0088 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4926 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0110 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0089 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0113 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0066 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0097 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0103 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e5242f610>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984a8bb2-0994-4e70-ed67-1e3b7284b8fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078e18f9-ad41-4358-d776-e0f36e0a51ef"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0427 - accuracy: 0.1377 - val_loss: 0.0350 - val_accuracy: 0.2121\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0335 - accuracy: 0.2552 - val_loss: 0.0322 - val_accuracy: 0.2843\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0317 - accuracy: 0.3016 - val_loss: 0.0312 - val_accuracy: 0.3105\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0304 - accuracy: 0.3373 - val_loss: 0.0302 - val_accuracy: 0.3443\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0295 - accuracy: 0.3608 - val_loss: 0.0293 - val_accuracy: 0.3666\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0287 - accuracy: 0.3813 - val_loss: 0.0291 - val_accuracy: 0.3676\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0279 - accuracy: 0.4003 - val_loss: 0.0285 - val_accuracy: 0.3916\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0273 - accuracy: 0.4144 - val_loss: 0.0276 - val_accuracy: 0.4074\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0268 - accuracy: 0.4265 - val_loss: 0.0271 - val_accuracy: 0.4215\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0263 - accuracy: 0.4388 - val_loss: 0.0273 - val_accuracy: 0.4142\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0260 - accuracy: 0.4466 - val_loss: 0.0268 - val_accuracy: 0.4265\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0257 - accuracy: 0.4547 - val_loss: 0.0264 - val_accuracy: 0.4407\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0253 - accuracy: 0.4640 - val_loss: 0.0263 - val_accuracy: 0.4373\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0250 - accuracy: 0.4718 - val_loss: 0.0265 - val_accuracy: 0.4314\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0248 - accuracy: 0.4785 - val_loss: 0.0260 - val_accuracy: 0.4510\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0246 - accuracy: 0.4807 - val_loss: 0.0260 - val_accuracy: 0.4495\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0244 - accuracy: 0.4855 - val_loss: 0.0261 - val_accuracy: 0.4476\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0242 - accuracy: 0.4897 - val_loss: 0.0257 - val_accuracy: 0.4584\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0240 - accuracy: 0.4935 - val_loss: 0.0260 - val_accuracy: 0.4537\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0238 - accuracy: 0.4980 - val_loss: 0.0256 - val_accuracy: 0.4588\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0237 - accuracy: 0.5018 - val_loss: 0.0264 - val_accuracy: 0.4401\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0235 - accuracy: 0.5082 - val_loss: 0.0254 - val_accuracy: 0.4647\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0233 - accuracy: 0.5109 - val_loss: 0.0256 - val_accuracy: 0.4593\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0232 - accuracy: 0.5136 - val_loss: 0.0264 - val_accuracy: 0.4426\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0231 - accuracy: 0.5176 - val_loss: 0.0255 - val_accuracy: 0.4653\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0229 - accuracy: 0.5207 - val_loss: 0.0256 - val_accuracy: 0.4649\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0228 - accuracy: 0.5224 - val_loss: 0.0259 - val_accuracy: 0.4567\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0227 - accuracy: 0.5295 - val_loss: 0.0255 - val_accuracy: 0.4701\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0226 - accuracy: 0.5294 - val_loss: 0.0256 - val_accuracy: 0.4656\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0225 - accuracy: 0.5313 - val_loss: 0.0257 - val_accuracy: 0.4669\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0224 - accuracy: 0.5317 - val_loss: 0.0253 - val_accuracy: 0.4702\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0223 - accuracy: 0.5368 - val_loss: 0.0254 - val_accuracy: 0.4759\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0222 - accuracy: 0.5395 - val_loss: 0.0256 - val_accuracy: 0.4653\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0221 - accuracy: 0.5414 - val_loss: 0.0260 - val_accuracy: 0.4608\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0220 - accuracy: 0.5443 - val_loss: 0.0253 - val_accuracy: 0.4741\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0219 - accuracy: 0.5460 - val_loss: 0.0254 - val_accuracy: 0.4784\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0218 - accuracy: 0.5488 - val_loss: 0.0249 - val_accuracy: 0.4848\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0217 - accuracy: 0.5529 - val_loss: 0.0252 - val_accuracy: 0.4776\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0216 - accuracy: 0.5557 - val_loss: 0.0253 - val_accuracy: 0.4797\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0215 - accuracy: 0.5548 - val_loss: 0.0252 - val_accuracy: 0.4778\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0214 - accuracy: 0.5579 - val_loss: 0.0253 - val_accuracy: 0.4740\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0213 - accuracy: 0.5597 - val_loss: 0.0253 - val_accuracy: 0.4801\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0212 - accuracy: 0.5613 - val_loss: 0.0257 - val_accuracy: 0.4716\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0211 - accuracy: 0.5643 - val_loss: 0.0252 - val_accuracy: 0.4783\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0211 - accuracy: 0.5684 - val_loss: 0.0251 - val_accuracy: 0.4803\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0210 - accuracy: 0.5688 - val_loss: 0.0255 - val_accuracy: 0.4710\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0209 - accuracy: 0.5698 - val_loss: 0.0253 - val_accuracy: 0.4767\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0208 - accuracy: 0.5722 - val_loss: 0.0255 - val_accuracy: 0.4683\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0208 - accuracy: 0.5739 - val_loss: 0.0254 - val_accuracy: 0.4724\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0207 - accuracy: 0.5747 - val_loss: 0.0254 - val_accuracy: 0.4850\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e52e92210>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe.\n",
        "\n",
        "The KDL results have large loss values, and their val_accuracy values sometimes explode into very long near-zero amounts."
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9213295b-8ab1-4b73-f18b-0bfed2ba361c"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5752 - accuracy: 0.2069 - val_loss: 2.3472 - val_accuracy: 0.2786\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2157 - accuracy: 0.3104 - val_loss: 2.0996 - val_accuracy: 0.3447\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0397 - accuracy: 0.3649 - val_loss: 2.0017 - val_accuracy: 0.3765\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9367 - accuracy: 0.3948 - val_loss: 1.9548 - val_accuracy: 0.3900\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8643 - accuracy: 0.4143 - val_loss: 1.9002 - val_accuracy: 0.4116\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8070 - accuracy: 0.4351 - val_loss: 1.8807 - val_accuracy: 0.4151\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7594 - accuracy: 0.4485 - val_loss: 1.8393 - val_accuracy: 0.4263\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7176 - accuracy: 0.4585 - val_loss: 1.8081 - val_accuracy: 0.4304\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6779 - accuracy: 0.4736 - val_loss: 1.7838 - val_accuracy: 0.4400\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6458 - accuracy: 0.4797 - val_loss: 1.7669 - val_accuracy: 0.4480\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6157 - accuracy: 0.4886 - val_loss: 1.8125 - val_accuracy: 0.4372\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5880 - accuracy: 0.4983 - val_loss: 1.7843 - val_accuracy: 0.4498\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5634 - accuracy: 0.5056 - val_loss: 1.7752 - val_accuracy: 0.4477\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5366 - accuracy: 0.5135 - val_loss: 1.7121 - val_accuracy: 0.4704\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5210 - accuracy: 0.5190 - val_loss: 1.7799 - val_accuracy: 0.4556\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4999 - accuracy: 0.5251 - val_loss: 1.7225 - val_accuracy: 0.4691\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4798 - accuracy: 0.5299 - val_loss: 1.7253 - val_accuracy: 0.4648\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4557 - accuracy: 0.5375 - val_loss: 1.7260 - val_accuracy: 0.4689\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4461 - accuracy: 0.5393 - val_loss: 1.7243 - val_accuracy: 0.4689\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4256 - accuracy: 0.5465 - val_loss: 1.7370 - val_accuracy: 0.4697\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4130 - accuracy: 0.5515 - val_loss: 1.7260 - val_accuracy: 0.4710\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3983 - accuracy: 0.5546 - val_loss: 1.7342 - val_accuracy: 0.4707\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3869 - accuracy: 0.5589 - val_loss: 1.7465 - val_accuracy: 0.4631\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3743 - accuracy: 0.5607 - val_loss: 1.7260 - val_accuracy: 0.4705\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3634 - accuracy: 0.5662 - val_loss: 1.7526 - val_accuracy: 0.4668\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e52e1dd10>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would."
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy(true, pred):\n",
        "    loss = np.sum(true * np.log(pred)) * -1 / len(true)\n",
        "    return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31721f0-d3dc-4035-a604-43f87b9da37b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3445815443992615\n",
            "tf.Tensor(0.34458154, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e00f01cc-0707-497a-c8e3-ae6c60a2d126"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5976 - accuracy: 0.1971 - val_loss: 2.3525 - val_accuracy: 0.2703\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.2300 - accuracy: 0.3095 - val_loss: 2.1518 - val_accuracy: 0.3340\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0861 - accuracy: 0.3535 - val_loss: 2.0614 - val_accuracy: 0.3603\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9970 - accuracy: 0.3788 - val_loss: 1.9827 - val_accuracy: 0.3787\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9176 - accuracy: 0.4013 - val_loss: 1.9835 - val_accuracy: 0.3830\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8590 - accuracy: 0.4193 - val_loss: 1.9093 - val_accuracy: 0.4021\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8114 - accuracy: 0.4334 - val_loss: 1.8828 - val_accuracy: 0.4147\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.7678 - accuracy: 0.4456 - val_loss: 1.8296 - val_accuracy: 0.4190\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7308 - accuracy: 0.4558 - val_loss: 1.8249 - val_accuracy: 0.4254\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.7055 - accuracy: 0.4609 - val_loss: 1.7945 - val_accuracy: 0.4374\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6750 - accuracy: 0.4722 - val_loss: 1.8167 - val_accuracy: 0.4287\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6492 - accuracy: 0.4805 - val_loss: 1.7876 - val_accuracy: 0.4350\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6294 - accuracy: 0.4863 - val_loss: 1.7655 - val_accuracy: 0.4459\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6111 - accuracy: 0.4937 - val_loss: 1.8230 - val_accuracy: 0.4355\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5904 - accuracy: 0.4992 - val_loss: 1.8080 - val_accuracy: 0.4348\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5706 - accuracy: 0.5037 - val_loss: 1.7389 - val_accuracy: 0.4591\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5553 - accuracy: 0.5095 - val_loss: 1.7498 - val_accuracy: 0.4517\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5410 - accuracy: 0.5114 - val_loss: 1.7526 - val_accuracy: 0.4503\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5239 - accuracy: 0.5183 - val_loss: 1.7441 - val_accuracy: 0.4533\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5093 - accuracy: 0.5223 - val_loss: 1.7828 - val_accuracy: 0.4477\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4950 - accuracy: 0.5276 - val_loss: 1.7366 - val_accuracy: 0.4600\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4853 - accuracy: 0.5289 - val_loss: 1.7369 - val_accuracy: 0.4597\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4686 - accuracy: 0.5333 - val_loss: 1.7900 - val_accuracy: 0.4492\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4639 - accuracy: 0.5344 - val_loss: 1.7553 - val_accuracy: 0.4588\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4535 - accuracy: 0.5378 - val_loss: 1.7325 - val_accuracy: 0.4627\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4444 - accuracy: 0.5405 - val_loss: 1.7889 - val_accuracy: 0.4516\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4335 - accuracy: 0.5433 - val_loss: 1.7396 - val_accuracy: 0.4597\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4248 - accuracy: 0.5450 - val_loss: 1.7720 - val_accuracy: 0.4540\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4168 - accuracy: 0.5478 - val_loss: 1.7522 - val_accuracy: 0.4627\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4094 - accuracy: 0.5484 - val_loss: 1.7619 - val_accuracy: 0.4669\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4005 - accuracy: 0.5535 - val_loss: 1.7586 - val_accuracy: 0.4616\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3979 - accuracy: 0.5534 - val_loss: 1.8103 - val_accuracy: 0.4493\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3857 - accuracy: 0.5578 - val_loss: 1.7376 - val_accuracy: 0.4707\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3827 - accuracy: 0.5592 - val_loss: 1.7890 - val_accuracy: 0.4593\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3729 - accuracy: 0.5596 - val_loss: 1.7573 - val_accuracy: 0.4700\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3666 - accuracy: 0.5637 - val_loss: 1.7616 - val_accuracy: 0.4626\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3613 - accuracy: 0.5635 - val_loss: 1.7874 - val_accuracy: 0.4584\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3549 - accuracy: 0.5649 - val_loss: 1.7467 - val_accuracy: 0.4665\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3441 - accuracy: 0.5700 - val_loss: 1.7589 - val_accuracy: 0.4661\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3425 - accuracy: 0.5707 - val_loss: 1.8038 - val_accuracy: 0.4651\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3356 - accuracy: 0.5713 - val_loss: 1.7791 - val_accuracy: 0.4599\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3317 - accuracy: 0.5727 - val_loss: 1.7991 - val_accuracy: 0.4588\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3286 - accuracy: 0.5725 - val_loss: 1.7749 - val_accuracy: 0.4679\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3208 - accuracy: 0.5756 - val_loss: 1.8041 - val_accuracy: 0.4677\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3177 - accuracy: 0.5754 - val_loss: 1.8115 - val_accuracy: 0.4650\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3143 - accuracy: 0.5785 - val_loss: 1.7877 - val_accuracy: 0.4654\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3023 - accuracy: 0.5810 - val_loss: 1.8020 - val_accuracy: 0.4665\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3002 - accuracy: 0.5795 - val_loss: 1.7898 - val_accuracy: 0.4716\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2968 - accuracy: 0.5828 - val_loss: 1.7906 - val_accuracy: 0.4670\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2955 - accuracy: 0.5831 - val_loss: 1.7922 - val_accuracy: 0.4698\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7e52877590>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?\n",
        "\n",
        "Multi-class means that there can only be one class attributed to an instance per classification (mutually exclusive), while Multi-label tasks can attribute multiple classes due to having multiple labels. We would use CCE to learn Multi-class classifications, and BCE for Multi-label ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 10\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "Binary Cross entropy is formally equal to the average of CCE in some two-category tasks. Either can be used in these situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "While they both perform generally the same calculation, they take in different forms of true labels/data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 9 Colab Notebook to your Github repository under \"Day 9\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}